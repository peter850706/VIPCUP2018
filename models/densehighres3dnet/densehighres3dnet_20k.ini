############################## input configuration sections
[image]
path_to_search = 
filename_contains = image
filename_not_contains = 
spatial_window_size = (192, 192, 48)
axcodes = (R,A,S)
interp_order = 3

[label]
path_to_search = 
filename_contains = label_GTV1
filename_not_contains =
spatial_window_size = (192, 192, 48)
axcodes = (R,A,S)
interp_order = 0

############################## system configuration sections
[SYSTEM]
cuda_devices = ""
num_threads = 4
num_gpus = 1
model_dir = 
dataset_split_file = 

[NETWORK]
name = densehighres3dnet.densehighres3dnet.DenseHighRes3DNet
activation_function = prelu
batch_size = 1
decay = 2e-4
reg_type = L2
#volume_padding_size = 
#volume_padding_mode = 
window_sampling = uniform
queue_length = 40

############################## volume-normalisation
### Setting normalisation=True enables the histogram-based normalisation
#normalisation = 
### Setting whitening=True enables the volume level normalisation computed by (I - mean(I))/std(I)
whitening = True
#histogram_ref_file = 
#norm_type = 
#cutoff = 
normalise_foreground_only = False
#multimod_foreground_type = 
#foreground_type = 

[TRAINING]
optimiser = adam
sample_per_volume = 8
lr = 3e-4
loss_type = Dice
starting_iter = 0
save_every_n = 20000
tensorboard_every_n = 1
max_iter = 20000
max_checkpoints = 100

### Validation during training
validation_every_n = 1
#validation_max_iter = 
#exclude_fraction_for_validation = 
#exclude_fraction_for_inference = 

### Data augmentation during training
do_elastic_deformation = True
num_ctrl_points = 4
deformation_sigma = 10
proportion_to_deform = 1
#rotation_angle = 
scaling_percentage = (-10.0, 10.0)
#random_flipping_axes = 

[INFERENCE]
spatial_window_size = (192, 192, 48)
border = (0, 0, 0)
inference_iter = -1
save_seg_dir = 
#output_postfix = 
output_interp_order = 0
dataset_to_infer = All

############################ custom configuration sections
[SEGMENTATION]
image = image
label = label
output_prob = True
num_classes = 2
label_normalisation = False
